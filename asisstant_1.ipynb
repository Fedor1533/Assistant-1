{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10bf3438",
   "metadata": {},
   "source": [
    "# Ассистент 1 - LM на основе n-грамм"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de848df",
   "metadata": {},
   "source": [
    "Перед вами - первое дополнительное задание повышенной сложности, в рамках которого вам предстоит начать разработку телеграм-бота с генеративными моделями.\n",
    "\n",
    "Цель данного ноутбука - помочь влиться в разработку ассистента. В данном ноутбуке написан код для \"обучения\" LM на основе n-грамм, для генерации с помощью нее текста, а также сохранение и загрузка модели и токенизатора.\n",
    "\n",
    "Относитесь к данному заданию максимально творчески - любую часть кода можно менять под ваши нужды и желания, можно оптимизировать, добавлять методы генерации, использовать любые данные, обучать сколь угодно \"большую\" модель. \n",
    "\n",
    "При этом вам стоит быть готовыми со всеми техническими проблеми справляться самому - именно так обычно происходит в реальной жизни в реальных проектах :) \n",
    "\n",
    "Поэтому отдельно подчеркну:\n",
    "* если что-то сломалось после ваших изменений - подразумевается, что вы сами найдете проблему и исправите\n",
    "* если вы ничего не трогали, но что-то не работает у нас - подразумевается, что вы сами найдете проблему и исправите :) \n",
    "\n",
    "Главный критерий выполнености данного задания - телеграм-бот, генерирующий текст и использующий обозначенный в задании подход (в случае данного ноутбука - n-граммная модель в любой ее реализации)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb8bd4",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca60e8",
   "metadata": {},
   "source": [
    "Для обучения качественной модели вам потребуются датасеты. В ноутбуке составлен маленький игрушечный датасет, вам для улучшения качества потребуется данные в большем количестве и более качественные, а также другие параметры модели и генерации (например, размер контекста побольше). \n",
    "\n",
    "С нормальным датасетом и правильными параметрами даже такой простой моделью можно добиться адекватного качества генерации текста (возможно не очень человечный, но вполне связный текст).\n",
    "\n",
    "Датасеты можно найти и выбрать тут (желательно на русском, вам так будет понятней качество и в целом полезней):\n",
    "https://huggingface.co/datasets\n",
    "  \n",
    "Можете найти наиболее интересный для себя датасет (можете сделать модель как смешной, так и полезной), либо выбрать любой из этих датасетов\n",
    "* https://huggingface.co/datasets/Den4ikAI/russian_dialogues\n",
    "* https://huggingface.co/datasets/Georgii/russianPoetry\n",
    "* https://huggingface.co/datasets/IgorVolochay/russian_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6c33a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T14:50:17.414585Z",
     "iopub.status.busy": "2024-02-21T14:50:17.413838Z",
     "iopub.status.idle": "2024-02-21T14:50:37.187697Z",
     "shell.execute_reply": "2024-02-21T14:50:37.186316Z",
     "shell.execute_reply.started": "2024-02-21T14:50:17.414553Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle \n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List, Dict, Optional, Iterable, Tuple\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df809daa",
   "metadata": {},
   "source": [
    "Токенизатор разбивает текст на слова. Можно попробовать другие способы токенизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90160389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:11.789756Z",
     "iopub.status.busy": "2024-02-21T15:03:11.788899Z",
     "iopub.status.idle": "2024-02-21T15:03:11.808584Z",
     "shell.execute_reply": "2024-02-21T15:03:11.807519Z",
     "shell.execute_reply.started": "2024-02-21T15:03:11.789725Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self,\n",
    "                 token_pattern: str = r'\\w+|[\\!\\?\\,\\.\\-\\:]',\n",
    "                 eos_token: str = '<EOS>',\n",
    "                 pad_token: str = '<PAD>',\n",
    "                 unk_token: str = '<UNK>'):\n",
    "        self.token_pattern = token_pattern\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        self.special_tokens = [self.eos_token, self.pad_token, self.unk_token]\n",
    "        self.vocab = None\n",
    "        self.inverse_vocab = None\n",
    "    \n",
    "    def text_preprocess(self, input_text: str) -> str:\n",
    "        \"\"\" Предобрабатываем один текст \"\"\"\n",
    "        input_text = str(input_text).lower()\n",
    "        input_text = re.sub(r'\\s+', ' ', input_text) # унифицируем пробелы\n",
    "        input_text = input_text.strip()\n",
    "        return input_text\n",
    "    \n",
    "    def build_vocab(self, corpus: List[str]) -> None:\n",
    "        assert len(corpus)\n",
    "        all_tokens = set()\n",
    "        for text in tqdm(corpus, desc='train corpus'):\n",
    "            all_tokens |= set(self._tokenize(text, append_eos_token=False))\n",
    "        self.vocab = {elem: ind for ind, elem in enumerate(all_tokens)}\n",
    "        special_tokens = [self.eos_token, self.unk_token, self.pad_token]\n",
    "        for token in special_tokens:\n",
    "            self.vocab[token] = len(self.vocab)\n",
    "        self.inverse_vocab = {ind: elem for elem, ind in self.vocab.items()}\n",
    "        return self\n",
    "        \n",
    "    def _tokenize(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
    "        text = self.text_preprocess(text)\n",
    "        tokens = re.findall(self.token_pattern, text)\n",
    "        if append_eos_token:\n",
    "            tokens.append(self.eos_token)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
    "        \"\"\" Токенизируем текст \"\"\"\n",
    "        tokens = self._tokenize(text, append_eos_token)\n",
    "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, input_ids: Iterable[int], remove_special_tokens: bool = False) -> str:\n",
    "        assert len(input_ids)\n",
    "        assert max(input_ids) < len(self.vocab) and min(input_ids) >= 0\n",
    "        tokens = []\n",
    "        for ind in input_ids:\n",
    "            token = self.inverse_vocab[ind]\n",
    "            if remove_special_tokens and token in self.special_tokens:\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        text = ' '.join( tokens )\n",
    "        return text\n",
    "    \n",
    "    def save(self, path: str) -> bool:\n",
    "        data = {\n",
    "            'token_pattern': self.token_pattern,\n",
    "            'eos_token': self.eos_token,\n",
    "            'pad_token': self.pad_token,\n",
    "            'unk_token': self.unk_token,\n",
    "            'special_tokens': self.special_tokens,\n",
    "            'vocab': self.vocab,\n",
    "            'inverse_vocab': self.inverse_vocab,\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as fout:\n",
    "            pickle.dump(data, fout)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def load(self, path: str) -> bool:\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "            \n",
    "        self.token_pattern = data['token_pattern']\n",
    "        self.eos_token = data['eos_token']\n",
    "        self.pad_token = data['pad_token']\n",
    "        self.unk_token = data['unk_token']\n",
    "        self.special_tokens = data['special_tokens']\n",
    "        self.vocab = data['vocab']\n",
    "        self.inverse_vocab = data['inverse_vocab']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999f620",
   "metadata": {},
   "source": [
    "Класс для задания параметров генерации, так удобней писать логику для валидации параметров и разные другие доп методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14d758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Тут можно задать любые параметры и их значения по умолчанию\n",
    "        Значения для стратегии декодирования decoding_strategy: ['max', 'top-p']\n",
    "        \"\"\"\n",
    "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
    "        self.max_tokens = kwargs.pop(\"max_tokens\", 32)\n",
    "        self.sample_top_p = kwargs.pop(\"sample_top_p\", 0.9)\n",
    "        self.decoding_strategy = kwargs.pop(\"decoding_strategy\", 'max')\n",
    "        self.remove_special_tokens = kwargs.pop(\"remove_special_tokens\", False)\n",
    "        self.validate()\n",
    "        \n",
    "    def validate(self):\n",
    "        \"\"\" Здесь можно валидировать параметры \"\"\"\n",
    "        if not (1.0 > self.sample_top_p > 0):\n",
    "            raise ValueError('sample_top_p')\n",
    "        if self.decoding_strategy not in ['max', 'top-p']:\n",
    "            raise ValueError('decoding_strategy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd923a4",
   "metadata": {},
   "source": [
    "Сама LM на основе n-грамм. Тут используется сглаживание Лапласа (можно поменять на метод backoff при желании), а также есть ряд параметров, сильно влияющий на качество генерации. Один из параметров генерации - стратегия генерации. \n",
    "\n",
    "Когда мы получили вероятности для следующего токена, мы по этим вероятностям хотим выбрать этот следующий токен.\n",
    "\n",
    "Можно просто семплировать из этого распределения - но тогда есть шанс, что будут очень маловероятные токены.\n",
    "\n",
    "Можно брать самый вероятный токен - но это плохо повлияет на разнообразие и \"человечность\" языка\n",
    "\n",
    "Можно воспользовать подходом top-p - семплировать только из тех токенов, которые наиболее вероятны (их вероятности суммируются в заданный p)\n",
    "\n",
    "Можно проверить, что top-p будет генерировать более интересный текст чем max\n",
    "\n",
    "Также обратите внимание на параметр температуры. В случае top-p и семплирования, чем больше делаешь температуру, тем меньше отличаются друг от друга вероятности (распределение стремится к равномерному, даже если исходное распределение имело вполне себе выраженные максимумы), и текст становится более случайным (и разнообразным)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "462316de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:21.268282Z",
     "iopub.status.busy": "2024-02-21T15:03:21.267567Z",
     "iopub.status.idle": "2024-02-21T15:03:21.292501Z",
     "shell.execute_reply": "2024-02-21T15:03:21.291323Z",
     "shell.execute_reply.started": "2024-02-21T15:03:21.268251Z"
    }
   },
   "outputs": [],
   "source": [
    "class StatLM:\n",
    "    def __init__(self, \n",
    "                 tokenizer: Tokenizer,\n",
    "                 context_size: int = 2,\n",
    "                 alpha: float = 0.1\n",
    "                ):\n",
    "        \n",
    "        assert context_size >= 2\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.n_gramms_stat = defaultdict(int)\n",
    "        self.nx_gramms_stat = defaultdict(int)\n",
    "        \n",
    "    def get_token_by_ind(self, ind: int) -> str:\n",
    "        return self.tokenizer.inverse_vocab.get(ind)\n",
    "    \n",
    "    def get_ind_by_token(self, token: str) -> int:\n",
    "        return self.tokenizer.vocab.get((token, self.tokenizer.vocab[self.unk_token]))\n",
    "        \n",
    "    def train(self, train_texts: List[str]):\n",
    "        for sentence in tqdm(train_texts, desc='train lines'):\n",
    "            sentence_ind = self.tokenizer.encode(sentence)\n",
    "            for i in range(len(sentence_ind) - self.context_size):\n",
    "                \n",
    "                seq = tuple(sentence_ind[i: i + self.context_size - 1])\n",
    "                self.n_gramms_stat[seq] += 1\n",
    "                \n",
    "                seq_x = tuple(sentence_ind[i: i + self.context_size])\n",
    "                self.nx_gramms_stat[seq_x] += 1\n",
    "                \n",
    "            seq = tuple(sentence_ind[len(sentence_ind) - self.context_size:])\n",
    "            self.n_gramms_stat[seq] += 1\n",
    "            \n",
    "    def sample_token(self, \n",
    "                     token_distribution: np.ndarray,\n",
    "                     generation_config: GenerationConfig) -> int:\n",
    "        if generation_config.decoding_strategy == 'max':\n",
    "            return token_distribution.argmax()\n",
    "        elif generation_config.decoding_strategy == 'top-p':\n",
    "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))),\n",
    "                                        reverse=True)\n",
    "            \n",
    "            total_proba = 0.0\n",
    "            tokens_to_sample = []\n",
    "            tokens_probas = []\n",
    "            for token_proba, ind in token_distribution:\n",
    "                tokens_to_sample.append(ind)\n",
    "                tokens_probas.append(token_proba)\n",
    "                total_proba += token_proba\n",
    "                if total_proba >= generation_config.sample_top_p:\n",
    "                    break\n",
    "            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n",
    "            tokens_probas = np.array(tokens_probas) / generation_config.temperature\n",
    "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
    "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown decoding strategy: {generation_config.decoding_strategy}')\n",
    "            \n",
    "    def save_stat(self, path: str) -> bool:\n",
    "        stat = {\n",
    "            'n_gramms_stat': self.n_gramms_stat,\n",
    "            'nx_gramms_stat': self.nx_gramms_stat,\n",
    "            'context_size': self.context_size,\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "        with open(path, 'wb') as fout:\n",
    "            pickle.dump(stat, fout)\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def load_stat(self, path: str) -> bool:\n",
    "        with open(path, 'rb') as fin:\n",
    "            stat = pickle.load(fin)\n",
    "            \n",
    "        self.n_gramms_stat = stat['n_gramms_stat']\n",
    "        self.nx_gramms_stat = stat['nx_gramms_stat']\n",
    "        self.context_size = stat['context_size']\n",
    "        self.alpha = stat['alpha']\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def get_stat(self) -> Dict[str, Dict]:\n",
    "        \n",
    "        n_token_stat, nx_token_stat = {}, {}\n",
    "        for token_inds, count in self.n_gramms_stat.items():\n",
    "            n_token_stat[self.tokenizer.decode(token_inds)] = count\n",
    "        \n",
    "        for token_inds, count in self.nx_gramms_stat.items():\n",
    "            nx_token_stat[self.tokenizer.decode(token_inds)] = count\n",
    "        \n",
    "        return {\n",
    "            'n gramms stat': self.n_gramms_stat,\n",
    "            'n+1 gramms stat': self.nx_gramms_stat,\n",
    "            'n tokens stat': n_token_stat,\n",
    "            'n+1 tokens stat': nx_token_stat,\n",
    "        }\n",
    "    \n",
    "    def _get_next_token(self, \n",
    "                        tokens: List[int],\n",
    "                        generation_config: GenerationConfig) -> (int, str):\n",
    "        print(f'Get next token: {self.tokenizer.decode(tokens, generation_config.remove_special_tokens)}')\n",
    "        denominator = self.n_gramms_stat.get(tuple(tokens), 0) + self.alpha * len(self.tokenizer.vocab)\n",
    "        print(f'Stat n: {self.n_gramms_stat.get(tuple(tokens), 0)}')\n",
    "        numerators = []\n",
    "        for ind in self.tokenizer.inverse_vocab:\n",
    "            if self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) > 0:\n",
    "                new_word = self.tokenizer.inverse_vocab[ind]\n",
    "                print(f'Stat nx: {self.nx_gramms_stat.get(tuple(tokens + [ind]), 0)} - {new_word}')\n",
    "            numerators.append(self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) + self.alpha)\n",
    "        \n",
    "        token_distribution = np.array(numerators) / denominator\n",
    "        max_proba_ind = self.sample_token(token_distribution, generation_config)\n",
    "        \n",
    "        next_token = self.tokenizer.inverse_vocab[max_proba_ind]\n",
    "        \n",
    "        return max_proba_ind, next_token\n",
    "            \n",
    "    def generate_token(self, \n",
    "                       text: str, \n",
    "                       generation_config: GenerationConfig\n",
    "                      ) -> Dict:\n",
    "        tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
    "        tokens = tokens[-self.context_size + 1:]\n",
    "        \n",
    "        max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
    "        \n",
    "        return {\n",
    "            'next_token': next_token,\n",
    "            'next_token_num': max_proba_ind,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def generate_text(self, text: str, \n",
    "                      generation_config: GenerationConfig\n",
    "                     ) -> Dict:\n",
    "        \n",
    "        all_tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
    "        input_tokens_len = len(all_tokens)\n",
    "        tokens = all_tokens[-self.context_size + 1:]\n",
    "        print(f'\\n----Input: {self.tokenizer.decode(tokens, generation_config.remove_special_tokens)}---\\n')\n",
    "        \n",
    "        next_token = None\n",
    "        while next_token != self.tokenizer.eos_token and len(all_tokens) < generation_config.max_tokens:\n",
    "            max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
    "            all_tokens.append(max_proba_ind)\n",
    "            tokens = all_tokens[-self.context_size + 1:]\n",
    "            print(f'Generation step result: {self.tokenizer.decode(tokens, generation_config.remove_special_tokens)}\\n')\n",
    "        \n",
    "        new_text = self.tokenizer.decode(all_tokens[input_tokens_len:], generation_config.remove_special_tokens)\n",
    "        \n",
    "        finish_reason = 'max tokens'\n",
    "        if all_tokens[-1] == self.tokenizer.vocab[self.tokenizer.eos_token]:\n",
    "            finish_reason = 'end of text'\n",
    "        \n",
    "        return {\n",
    "            'all_tokens': all_tokens,\n",
    "            'total_text': new_text,\n",
    "            'finish_reason': finish_reason\n",
    "        }\n",
    "    \n",
    "    def generate(self, text: str, generation_config: Dict) -> str:\n",
    "        return self.generate_text(text, generation_config)['total_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f60f19",
   "metadata": {},
   "source": [
    "Эта функция напрямую используется в телеграм боте для получения модели и конфига генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cfdad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    config = {\n",
    "        'temperature': 1.0,\n",
    "        'max_tokens': 32,\n",
    "        'sample_top_p': 0.9,\n",
    "        'decoding_strategy': 'top-p',\n",
    "    }\n",
    "\n",
    "    stat_lm_path = 'models/stat_lm/stat_lm.pkl'\n",
    "    tokenizer_path = 'models/stat_lm/tokenizer.pkl'\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.load(tokenizer_path)\n",
    "        \n",
    "    stat_lm = StatLM(tokenizer)\n",
    "    stat_lm.load_stat(stat_lm_path)\n",
    "\n",
    "    generation_config = GenerationConfig(temperature=config['temperature'],\n",
    "                                         max_tokens=config['max_tokens'],\n",
    "                                         sample_top_p=config['sample_top_p'],\n",
    "                                         decoding_strategy=config['decoding_strategy'],\n",
    "                                         remove_special_tokens=True)\n",
    "\n",
    "    kwargs = {'generation_config': generation_config}\n",
    "    return stat_lm, kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85b767",
   "metadata": {},
   "source": [
    "### Обучаем на игрушечных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674d63a",
   "metadata": {},
   "source": [
    "Для демонстрации того, что происходит, возьмем несколько коротких цитат Джейсона Стэтхема отсюда:\n",
    "\n",
    "https://dzen.ru/a/ZRFaGN_gKhX6xTWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e19a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(ds_name: str = 'Den4ikAI/russian_dialogues', ds_size=None, split='train'): \n",
    "    dataset = load_dataset(ds_name, split=split)\n",
    "    return pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e81917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>как дела?</td>\n",
       "      <td>там хорошо</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>вы кефир пачему не кушаете, не любите?</td>\n",
       "      <td>я ряженку лучше люблю.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>если в расходную накладную забить дури и выкур...</td>\n",
       "      <td>особенно когда придет комиссия проверять докум...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>покажись в шапке</td>\n",
       "      <td>ды щаз приветик</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>давай не будем об этом</td>\n",
       "      <td>давай поговорим о чем-нибудь другом</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477316</th>\n",
       "      <td>а ваша гармонь отчего поет?</td>\n",
       "      <td>нет, просто наслаждаюсь пением.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477317</th>\n",
       "      <td>а кто или что мешает вам быть истинно счастливым?</td>\n",
       "      <td>да. это просто. надо радоваться каждому мгнове...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477318</th>\n",
       "      <td>как сделать визу в нигерию без справки с работы?</td>\n",
       "      <td>безработных туда не пускают, значит никак</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477319</th>\n",
       "      <td>есть универсальный способ проверки состояния м...</td>\n",
       "      <td>да выжать и отпустить сцепление на заведеной н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477320</th>\n",
       "      <td>завтра на работу не поеду, что то я раскисла.</td>\n",
       "      <td>а шо случилось?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2477321 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  question  \\\n",
       "0                                                как дела?   \n",
       "1                   вы кефир пачему не кушаете, не любите?   \n",
       "2        если в расходную накладную забить дури и выкур...   \n",
       "3                                         покажись в шапке   \n",
       "4                                   давай не будем об этом   \n",
       "...                                                    ...   \n",
       "2477316                        а ваша гармонь отчего поет?   \n",
       "2477317  а кто или что мешает вам быть истинно счастливым?   \n",
       "2477318   как сделать визу в нигерию без справки с работы?   \n",
       "2477319  есть универсальный способ проверки состояния м...   \n",
       "2477320      завтра на работу не поеду, что то я раскисла.   \n",
       "\n",
       "                                                    answer  relevance  \n",
       "0                                               там хорошо          0  \n",
       "1                                   я ряженку лучше люблю.          1  \n",
       "2        особенно когда придет комиссия проверять докум...          1  \n",
       "3                                          ды щаз приветик          0  \n",
       "4                      давай поговорим о чем-нибудь другом          1  \n",
       "...                                                    ...        ...  \n",
       "2477316                    нет, просто наслаждаюсь пением.          0  \n",
       "2477317  да. это просто. надо радоваться каждому мгнове...          0  \n",
       "2477318          безработных туда не пускают, значит никак          1  \n",
       "2477319  да выжать и отпустить сцепление на заведеной н...          1  \n",
       "2477320                                    а шо случилось?          0  \n",
       "\n",
       "[2477321 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts = get_dataset()\n",
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15704fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>вы кефир пачему не кушаете, не любите?</td>\n",
       "      <td>я ряженку лучше люблю.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>если в расходную накладную забить дури и выкур...</td>\n",
       "      <td>особенно когда придет комиссия проверять докум...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>давай не будем об этом</td>\n",
       "      <td>давай поговорим о чем-нибудь другом</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>препарат для лечения сильно понижает давление....</td>\n",
       "      <td>чтоб не сильно? или что? препарат принимай и к...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>мужчина, если ты занюхиваешь волосами соседки,...</td>\n",
       "      <td>предпочитаю соседкиными пирогами закусывать. -</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477310</th>\n",
       "      <td>вы много позиций можете применить в сексе за о...</td>\n",
       "      <td>ха успеваем еще как, че топатся на одном месте</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477311</th>\n",
       "      <td>может ли взрослый человек разочаровываться? вз...</td>\n",
       "      <td>взрослый-самодостаточный. нет</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477315</th>\n",
       "      <td>еще б ты коня не видел, хм</td>\n",
       "      <td>ахах нихуясе камень в мой огород</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477318</th>\n",
       "      <td>как сделать визу в нигерию без справки с работы?</td>\n",
       "      <td>безработных туда не пускают, значит никак</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477319</th>\n",
       "      <td>есть универсальный способ проверки состояния м...</td>\n",
       "      <td>да выжать и отпустить сцепление на заведеной н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1382615 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  question  \\\n",
       "1                   вы кефир пачему не кушаете, не любите?   \n",
       "2        если в расходную накладную забить дури и выкур...   \n",
       "4                                   давай не будем об этом   \n",
       "5        препарат для лечения сильно понижает давление....   \n",
       "6        мужчина, если ты занюхиваешь волосами соседки,...   \n",
       "...                                                    ...   \n",
       "2477310  вы много позиций можете применить в сексе за о...   \n",
       "2477311  может ли взрослый человек разочаровываться? вз...   \n",
       "2477315                         еще б ты коня не видел, хм   \n",
       "2477318   как сделать визу в нигерию без справки с работы?   \n",
       "2477319  есть универсальный способ проверки состояния м...   \n",
       "\n",
       "                                                    answer  relevance  \n",
       "1                                   я ряженку лучше люблю.          1  \n",
       "2        особенно когда придет комиссия проверять докум...          1  \n",
       "4                      давай поговорим о чем-нибудь другом          1  \n",
       "5        чтоб не сильно? или что? препарат принимай и к...          1  \n",
       "6           предпочитаю соседкиными пирогами закусывать. -          1  \n",
       "...                                                    ...        ...  \n",
       "2477310     ха успеваем еще как, че топатся на одном месте          1  \n",
       "2477311                      взрослый-самодостаточный. нет          1  \n",
       "2477315                   ахах нихуясе камень в мой огород          1  \n",
       "2477318          безработных туда не пускают, значит никак          1  \n",
       "2477319  да выжать и отпустить сцепление на заведеной н...          1  \n",
       "\n",
       "[1382615 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_texts = train_texts[train_texts['relevance'] == 1]\n",
    "relevant_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85c33b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650000,\n",
       " ['вы кефир пачему не кушаете, не любите? я ряженку лучше люблю.',\n",
       "  'если в расходную накладную забить дури и выкурить, то получится приходный документ? особенно когда придет комиссия проверять документацию',\n",
       "  'давай не будем об этом давай поговорим о чем-нибудь другом',\n",
       "  'препарат для лечения сильно понижает давление. что порекомендуете? чтоб не сильно? или что? препарат принимай и кофе пей',\n",
       "  'мужчина, если ты занюхиваешь волосами соседки, то какой аромат предпочитаешь? предпочитаю соседкиными пирогами закусывать. -'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qustino_answer_texts = (relevant_texts['question'] + ' ' + relevant_texts['answer'])[:650000].tolist()\n",
    "len(qustino_answer_texts), qustino_answer_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e92a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:25.218697Z",
     "iopub.status.busy": "2024-02-21T15:03:25.218225Z",
     "iopub.status.idle": "2024-02-21T15:03:25.223271Z",
     "shell.execute_reply": "2024-02-21T15:03:25.222261Z",
     "shell.execute_reply.started": "2024-02-21T15:03:25.218665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ff3275e53d49d79e1471715914cb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train corpus:   0%|          | 0/650000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = Tokenizer().build_vocab(qustino_answer_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f61a4e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:25.398291Z",
     "iopub.status.busy": "2024-02-21T15:03:25.397981Z",
     "iopub.status.idle": "2024-02-21T15:03:25.406154Z",
     "shell.execute_reply": "2024-02-21T15:03:25.405134Z",
     "shell.execute_reply.started": "2024-02-21T15:03:25.398265Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385351,\n",
       " {'напргяет': 2200,\n",
       "  'трухлявыми': 2201,\n",
       "  'чпокаться': 2202,\n",
       "  'мономаха': 2203,\n",
       "  'встретившие': 2204,\n",
       "  'каратисты': 2205,\n",
       "  'прилог': 2206,\n",
       "  'меркантильно': 2207,\n",
       "  'продизенфицирует': 2208,\n",
       "  'отлежусь': 2209})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab), dict(list(tokenizer.vocab.items())[2200:2210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ef0948d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:29.936948Z",
     "iopub.status.busy": "2024-02-21T15:03:29.936563Z",
     "iopub.status.idle": "2024-02-21T15:03:29.957657Z",
     "shell.execute_reply": "2024-02-21T15:03:29.956641Z",
     "shell.execute_reply.started": "2024-02-21T15:03:29.936919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4081a68429eb434fba146b374fde91aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train lines:   0%|          | 0/650000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# класс, который позволяем строить и использовать языковую модель на основе n-грамм\n",
    "stat_lm = StatLM(tokenizer, context_size=4, alpha=0.01)\n",
    "\n",
    "# \"обучаем\" модель - считаем статистики\n",
    "stat_lm.train(qustino_answer_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b395182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seq_0: вы кефир пачему  stat: 1\n",
      " Seq_1: кефир пачему не  stat: 1\n",
      " Seq_2: пачему не кушаете  stat: 1\n",
      " Seq_3: не кушаете ,  stat: 1\n",
      " Seq_4: кушаете , не  stat: 1\n",
      " Seq_5: , не любите  stat: 7\n",
      " Seq_6: не любите ?  stat: 88\n",
      " Seq_7: любите ? я  stat: 86\n",
      " Seq_8: ? я ряженку  stat: 1\n",
      " Seq_9: я ряженку лучше  stat: 1\n",
      " Seq_10: ряженку лучше люблю  stat: 1\n",
      " Seq_11: лучше люблю .  stat: 1\n",
      " Seq_12: если в расходную  stat: 1\n",
      " Seq_13: в расходную накладную  stat: 1\n",
      " Seq_14: расходную накладную забить  stat: 1\n",
      " Seq_15: накладную забить дури  stat: 1\n",
      " Seq_16: забить дури и  stat: 1\n",
      " Seq_17: дури и выкурить  stat: 1\n",
      " Seq_18: и выкурить ,  stat: 1\n",
      " Seq_19: выкурить , то  stat: 1\n",
      " Seq_20: , то получится  stat: 31\n",
      " Seq_21: то получится приходный  stat: 1\n"
     ]
    }
   ],
   "source": [
    "for i, (tokens, stat) in enumerate(stat_lm.n_gramms_stat.items()):\n",
    "    print(f' Seq_{i}: {tokenizer.decode(tokens, True)}  stat: {stat}')\n",
    "    if i > 20: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b515d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(temperature = 1.0, max_tokens = 16,\n",
    "                                     sample_top_p = 0.01, decoding_strategy = 'top-p',\n",
    "                                     remove_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e73f538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Input: расскажи историю---\n",
      "\n",
      "Get next token: расскажи историю\n",
      "Stat n: 0\n",
      "Generation step result: расскажи историю превет\n",
      "\n",
      "Get next token: расскажи историю превет\n",
      "Stat n: 0\n",
      "Generation step result: историю превет неньютоновской\n",
      "\n",
      "Get next token: историю превет неньютоновской\n",
      "Stat n: 0\n",
      "Generation step result: превет неньютоновской граммовесли\n",
      "\n",
      "Get next token: превет неньютоновской граммовесли\n",
      "Stat n: 0\n",
      "Generation step result: неньютоновской граммовесли пиздюк\n",
      "\n",
      "Get next token: неньютоновской граммовесли пиздюк\n",
      "Stat n: 0\n",
      "Generation step result: граммовесли пиздюк пассажирке\n",
      "\n",
      "Get next token: граммовесли пиздюк пассажирке\n",
      "Stat n: 0\n",
      "Generation step result: пиздюк пассажирке организавываем\n",
      "\n",
      "Get next token: пиздюк пассажирке организавываем\n",
      "Stat n: 0\n",
      "Generation step result: пассажирке организавываем радиоприемники\n",
      "\n",
      "Get next token: пассажирке организавываем радиоприемники\n",
      "Stat n: 0\n",
      "Generation step result: организавываем радиоприемники мейн\n",
      "\n",
      "Get next token: организавываем радиоприемники мейн\n",
      "Stat n: 0\n",
      "Generation step result: радиоприемники мейн удавольствием\n",
      "\n",
      "Get next token: радиоприемники мейн удавольствием\n",
      "Stat n: 0\n",
      "Generation step result: мейн удавольствием техонечько\n",
      "\n",
      "Get next token: мейн удавольствием техонечько\n",
      "Stat n: 0\n",
      "Generation step result: удавольствием техонечько оттолкнет\n",
      "\n",
      "Get next token: удавольствием техонечько оттолкнет\n",
      "Stat n: 0\n",
      "Generation step result: техонечько оттолкнет едроссня\n",
      "\n",
      "Get next token: техонечько оттолкнет едроссня\n",
      "Stat n: 0\n",
      "Generation step result: оттолкнет едроссня приувеличивает\n",
      "\n",
      "Get next token: оттолкнет едроссня приувеличивает\n",
      "Stat n: 0\n",
      "Generation step result: едроссня приувеличивает ростове\n",
      "\n",
      "Расскажи историю - превет неньютоновской граммовесли пиздюк пассажирке организавываем радиоприемники мейн удавольствием техонечько оттолкнет едроссня приувеличивает ростове\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Расскажи историю\"\n",
    "print(f\"{test_text} - {stat_lm.generate(test_text, generation_config)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d054a1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save('models/stat_lm/tokenizer.pkl')\n",
    "stat_lm.save_stat('models/stat_lm/stat_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bcef7",
   "metadata": {},
   "source": [
    "Тут мы для токенизатора сохраняем только спецтокены и словарь, для модели - параметры и статистики n-грамм и n+1-грамм. Потом в телеграм боте подгружаем именно эти параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf63a9",
   "metadata": {},
   "source": [
    "Когда обучите модель на большом датасете, советую посмотреть на распределение вероятностей для следующего слова при разных входах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e659ff6",
   "metadata": {},
   "source": [
    "### смотрим как конструировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0ad87959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как дела ? попутчик запинаем помогите уркаган явиться наезд хартенбраунгевратенштайзенгорбейстраут слоем нажратое накрылись построен бобро пересвет растрезвонит подробными подъезд увеличенными морозильнике внушительным приведены жаргон выведенный засахарилось яшике прозондирую херсонаь бологом пвх рязанский'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, kwargs = construct_model()\n",
    "\n",
    "model.generate(\"Как дела?\", **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517748d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp_6",
   "language": "python",
   "name": "nlp_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
